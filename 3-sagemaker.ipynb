{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Using Llama-Index with Amazon SageMaker\n",
    "\n",
    "![](./images/sage-riding-llama.png)\n",
    "\n",
    "Before being able to use Llama-Index with Amazon SageMaker, we will have to deploy a model for embedding and a model for generating the text. In my case, I will be using a model from the HuggingFace Hub for the embeddings ([intfloat/e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct)), and a model from SageMaker JumpStart for the generative LLM ([mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)). Consistency between model family is not required, but it is a nice touch :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 sagemaker -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/dggallit/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AMZN-MAC-User to get Role path.\n"
     ]
    }
   ],
   "source": [
    "#### SAGEMAKER SETUP ####\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='MyExecutionRole')['Role']['Arn']\n",
    "\t\n",
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "#### EMBEDDING MODEL DEPLOYMENT ####\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. <https://huggingface.co/models>\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'intfloat/multilingual-e5-large-instruct', # model_id from hf.co/models\n",
    "  'HF_TASK':'feature-extraction', # NLP task you want to use for predictions\n",
    "  'SM_NUM_GPUS': '1',\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub, # configuration for loading model from Hub\n",
    "    role=role, # iam role with permissions to create an Endpoint\n",
    "    py_version='py310',\n",
    "    transformers_version=\"4.37.0\", # transformers version used\n",
    "    pytorch_version=\"2.1.0\", # pytorch version used\n",
    ")\n",
    "\n",
    "embedding_predictor = huggingface_model.deploy(\n",
    "    endpoint_name=sagemaker.utils.name_from_base(\"intfloat-multilingual-e5-large-instruct\"),\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.4xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "#### GENERATIVE MODEL DEPLOYMENT ####\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id = \"huggingface-llm-mixtral-8x7b-instruct\", role=role)\n",
    "predictor = model.deploy(endpoint_name=sagemaker.utils.name_from_base(\"llm-mixtral-8x7b-instruct\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up SageMaker Endpoints as embedding model and LLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.sagemaker_endpoint import SageMakerLLM\n",
    "from llama_index.embeddings.sagemaker_endpoint import SageMakerEmbedding\n",
    "\n",
    "# Model that will be used to generate the embeddings\n",
    "Settings.embed_model = SageMakerEmbedding(endpoint_name=embedding_predictor.endpoint_name, embed_batch_size=2)\n",
    "# Model that will be used to generate the answer given the most relevant chunks\n",
    "Settings.llm = SageMakerLLM(endpoint_name=predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondo le informazioni fornite nel contesto, Pinocchio è un ragazzaccio, un vagabondo e un vero rompicollo, come descritto dalle parole di un suo compagno di scuola. Tuttavia, quando il burattino protagonista cerca di difendere Pinocchio, dicendo che è un gran buon figliuolo, pieno di voglia di studiare, obbediente e affezionato al suo babbo, il suo naso si allunga, rivelando che in realtà sta mentendo. Quindi Pinocchio sembra essere un ragazzo disubbidiente e svogliato, che invece di andare a scuola, va in giro con i compagni a fare lo sbarazzino.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Step 1 - Load Data\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "# Step 2 - Index Data\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=400, chunk_overlap=50), # Max tokens size is 512 for this model\n",
    "    ], \n",
    "    show_progress=True,\n",
    ")\n",
    "# Step 3 - Query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Chi è Pinocchio?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Predictors\n",
    "embedding_predictor.delete_predictor()\n",
    "predictor.delete_predictor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
